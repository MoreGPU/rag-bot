{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create chat function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/openai-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# OPENAI CONFIGURATION\n",
    "openai_key = os.environ.get('openai_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class openai_chat:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 openai_key:str,\n",
    "                 model:str=\"gpt-3.5-turbo\",\n",
    "                 system_message:str=None\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.system_message = system_message\n",
    "        \n",
    "        # initialize openai client\n",
    "        self.client = OpenAI(\n",
    "            api_key=openai_key \n",
    "        )\n",
    "        \n",
    "    def _create_system_message(self, system_message:str=None):\n",
    "        if system_message==None: \n",
    "            system_message = \"You are an assistant here to answer questions about the ebook: 'MLOps for Dummies: Databricks Special Edition'\" \n",
    "        return {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":system_message\n",
    "        }\n",
    "        \n",
    "    def _create_human_message(self, human_message:str):\n",
    "        return {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":human_message\n",
    "        }    \n",
    "        \n",
    "    def _create_assistant_message(self, assistant_message:str):\n",
    "        return {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":assistant_message\n",
    "        }\n",
    "        \n",
    "    def __call__(self, prompt:str, memory:List[dict]=None):\n",
    "        \n",
    "        if memory == None:\n",
    "            self.messages = []\n",
    "            self.messages.append( self._create_system_message(self.system_message) )\n",
    "        else:\n",
    "            self.messages = memory\n",
    "        \n",
    "        human_message = self._create_human_message(prompt)\n",
    "        self.messages.append( human_message )\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            messages = self.messages,\n",
    "            model=self.model\n",
    "        )\n",
    "        response = chat_completion.choices[0].message.content \n",
    "        assistant_message = self._create_assistant_message(response)\n",
    "        self.messages.append(assistant_message)\n",
    "        memory = self.messages\n",
    "\n",
    "        return response, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(context:List[str], query:str):\n",
    "    context_string = '\\n\\n'.join(context)\n",
    "    prompt = f\"\"\"\n",
    "    Answer the query based on the provided context. If the answer is not provided in the context, answer \"I don't know\".\n",
    "    \n",
    "    Context:\n",
    "    {context_string}\n",
    "    \n",
    "    \n",
    "    Query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLOps, which stands for \"Machine Learning Operations,\" is a practice that combines machine learning and DevOps processes to operationalize and manage the end-to-end lifecycle of machine learning models. MLOps aims to enable organizations to build, deploy, monitor, and scale machine learning models effectively and efficiently, ensuring their production readiness and improving the overall model lifecycle management. It involves integrating machine learning workflows with software engineering practices to ensure smooth collaboration between data scientists, data engineers, and IT operations teams.\n",
      "-----------------------------\n",
      "MLOps is the practice of integrating machine learning and DevOps methodologies to effectively manage and operationalize the lifecycle of machine learning models, enabling organizations to efficiently deploy, monitor, and scale their models in a production environment.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chat = openai_chat(openai_key)\n",
    "    \n",
    "    response, memory = chat(\n",
    "        prompt=\"what is mlops?\", \n",
    "        memory=None\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "    print('-----------------------------')\n",
    "    response, memory = chat(\n",
    "        prompt=\"summarize that in one sentence.\", \n",
    "        memory=memory\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
