{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "from config import *\n",
    "from utils.data_utils import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_creds = AzureKeyCredential(searchkey)\n",
    "storage_creds = storagekey\n",
    "openai_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "       \n",
    "    @staticmethod\n",
    "    def get_document_text(pdf_path):\n",
    "        \"\"\" \n",
    "        Extracts text from pdf and returns text and page map\n",
    "        \n",
    "        :param pdf_path: Location of pdf file\n",
    "        :return: page_text: String of concatenated text\n",
    "        :return: page_map: dictionary mapping offset to page number\n",
    "        \"\"\"\n",
    "        from pypdf import PdfReader\n",
    "        offset = 0\n",
    "        all_text = \"\"\n",
    "        page_map = {}\n",
    "        reader = PdfReader(pdf_path)\n",
    "        pages = reader.pages \n",
    "        for page_num, page in enumerate(pages):\n",
    "            page_text = page.extract_text()\n",
    "            all_text += page_text\n",
    "            page_map[offset] = page_num\n",
    "            offset += len(page_text.split(' '))\n",
    "        return all_text, page_map\n",
    "    \n",
    "class Chunker():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size = 500,\n",
    "        overlap = 50,\n",
    "        separator = \" \" \n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap \n",
    "        self.separator = separator\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        splits = text.split(' ')\n",
    "        return splits\n",
    "            \n",
    "    def get_chunk_indices(self, text):\n",
    "        splits = self.split_text(text)\n",
    "        chunk_indices = [ (i, i+self.chunk_size) for i in range(0, len(splits)-self.overlap, self.chunk_size-self.overlap)]\n",
    "        return chunk_indices       \n",
    "    \n",
    "    def create_chunks(self, text, page_map):\n",
    "        \n",
    "        def find_page(start_idx, page_map):\n",
    "            offset = max(x for x in page_map.keys() if x <= start_idx)\n",
    "            page_num = page_map[offset]\n",
    "            return page_num\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_indices = self.get_chunk_indices(text)            \n",
    "        for start_idx, end_idx in chunk_indices:\n",
    "            page_num = find_page(start_idx, page_map)\n",
    "                \n",
    "            chunk = text.split(self.separator)[start_idx:end_idx]\n",
    "            chunk = self.separator.join(chunk)\n",
    "            chunks.append( (chunk, page_num))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "class Embedder:\n",
    "    def __init__(\n",
    "        self, \n",
    "        key=None,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    ):\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        self.embedder = OpenAIEmbeddings(openai_api_key=key, model=model)\n",
    "        \n",
    "    def embed_in_batches(self, chunks, batch_size=16):\n",
    "        num_batches = math.ceil(len(chunks) / batch_size)\n",
    "        embeddings = []\n",
    "        for i in range(num_batches):            \n",
    "            batch = chunks[i*batch_size:i*batch_size+batch_size]\n",
    "            embeddings_batch = self.embedder.embed_documents(batch)\n",
    "            embeddings += embeddings_batch\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_single_document(self, text):\n",
    "        embedding = self.embedder.embed_documents([text])\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "def create_sections(chunks):\n",
    "    for i, (text, page_num) in enumerate(chunks):\n",
    "        yield {\n",
    "            \"id\":str(i),\n",
    "            \"content\":text,\n",
    "            \"embedding\":embedder.embed_single_document(text)[0],\n",
    "            \"sourcepage\":str(page_num),\n",
    "            \"sourcefile\":FILE_PATH.split('/')[-1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PDFExtractor()\n",
    "chunker = Chunker(chunk_size=500, overlap=100, separator=\" \")\n",
    "\n",
    "text, page_map = extractor.get_document_text(FILE_PATH)\n",
    "chunks = chunker.create_chunks(text, page_map)\n",
    "sections = create_sections(chunks)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index(index):\n",
    "    \n",
    "    print(f\"Ensuring search index {index} exists\")\n",
    "    index_client = SearchIndexClient(\n",
    "        endpoint=f\"https://{searchservice}.search.windows.net/\",\n",
    "        credential=search_creds \n",
    "    )\n",
    "    \n",
    "    if index not in index_client.list_index_names():\n",
    "        \n",
    "        # configure the index \n",
    "        fields = [\n",
    "            SimpleField(\n",
    "                name=\"id\", \n",
    "                type=SearchFieldDataType.String, \n",
    "                key=True,\n",
    "                sortable=True,\n",
    "                filterable=True,\n",
    "                facetable=True\n",
    "                ),\n",
    "            SearchableField(\n",
    "                name=\"content\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                analyzer_name=\"en.microsoft\" \n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"embedding\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True,\n",
    "                vector_search_dimensions=1536,\n",
    "                vector_search_profile_name=\"mlops-vector-profile\"\n",
    "            ),\n",
    "            SimpleField(\n",
    "                name=\"sourcepage\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                filterable=True,\n",
    "                facetable=True \n",
    "            ),\n",
    "            SimpleField(\n",
    "                name=\"sourcefile\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                filterable=False,\n",
    "                facetable=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # configure the vector search configuration\n",
    "        vector_search = VectorSearch(\n",
    "            profiles=[VectorSearchProfile(name=\"mlops-vector-profile\", algorithm_configuration_name=\"mlops-vector-search-algo\")],\n",
    "            algorithms=[HnswAlgorithmConfiguration(name=\"mlops-vector-search-algo\")]\n",
    "        )\n",
    "        \n",
    "        # configure semantic search\n",
    "        semantic_config = SemanticConfiguration(\n",
    "            name=\"mlops-semantic-config\",\n",
    "            prioritized_fields=SemanticPrioritizedFields(\n",
    "                content_fields=[SemanticField(field_name=\"content\")]\n",
    "            )\n",
    "        )\n",
    "        semantic_search = SemanticSearch(\n",
    "            configurations=[semantic_config]\n",
    "        )\n",
    "        \n",
    "        # create the search index with vector and semantic settings\n",
    "        index = SearchIndex(\n",
    "            name=index,\n",
    "            fields=fields,\n",
    "            vector_search=vector_search,\n",
    "            semantic_search=semantic_search \n",
    "        )\n",
    "        result = index_client.create_or_update_index(index)\n",
    "        print(f\"{result.name} created\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Search index {index} already exists\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring search index mlops-rag exists\n",
      "mlops-rag created\n"
     ]
    }
   ],
   "source": [
    "create_search_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
