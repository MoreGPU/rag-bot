{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# AZURE AI SEARCH CREDENTIALS\n",
    "searchservice = os.environ.get('searchservice')\n",
    "index = os.environ.get('index')\n",
    "searchkey = os.environ.get('searchkey')\n",
    "\n",
    "# OPENAI CONFIGURATION\n",
    "openai_key = os.environ.get('openai_key')\n",
    "\n",
    "# DATA CONFIGURATION\n",
    "filepath = os.environ.get('filepath')\n",
    "\n",
    "# set credentials\n",
    "search_creds = AzureKeyCredential(searchkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractor:\n",
    "       \n",
    "    @staticmethod\n",
    "    def get_document_text(pdf_path):\n",
    "        \"\"\" \n",
    "        Extracts text from pdf and returns text and page map\n",
    "        \n",
    "        :param pdf_path: Location of pdf file\n",
    "        :return: page_text: String of concatenated text\n",
    "        :return: page_map: dictionary mapping offset to page number\n",
    "        \"\"\"\n",
    "        from pypdf import PdfReader\n",
    "        offset = 0\n",
    "        all_text = \"\"\n",
    "        page_map = {}\n",
    "        reader = PdfReader(pdf_path)\n",
    "        pages = reader.pages \n",
    "        for page_num, page in enumerate(pages):\n",
    "            page_text = page.extract_text()\n",
    "            all_text += page_text\n",
    "            page_map[offset] = page_num\n",
    "            offset += len(page_text.split(' '))\n",
    "        return all_text, page_map\n",
    "    \n",
    "class Chunker():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size = 500,\n",
    "        overlap = 50,\n",
    "        separator = \" \" \n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap \n",
    "        self.separator = separator\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        splits = text.split(' ')\n",
    "        return splits\n",
    "            \n",
    "    def get_chunk_indices(self, text):\n",
    "        splits = self.split_text(text)\n",
    "        chunk_indices = [ (i, i+self.chunk_size) for i in range(0, len(splits)-self.overlap, self.chunk_size-self.overlap)]\n",
    "        return chunk_indices       \n",
    "    \n",
    "    def create_chunks(self, text, page_map):\n",
    "        \n",
    "        def find_page(start_idx, page_map):\n",
    "            offset = max(x for x in page_map.keys() if x <= start_idx)\n",
    "            page_num = page_map[offset]\n",
    "            return page_num\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_indices = self.get_chunk_indices(text)            \n",
    "        for start_idx, end_idx in chunk_indices:\n",
    "            page_num = find_page(start_idx, page_map)\n",
    "                \n",
    "            chunk = text.split(self.separator)[start_idx:end_idx]\n",
    "            chunk = self.separator.join(chunk)\n",
    "            chunks.append( (chunk, page_num))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "class Embedder:\n",
    "    def __init__(\n",
    "        self, \n",
    "        key=None,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    ):\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        self.embedder = OpenAIEmbeddings(openai_api_key=key, model=model)\n",
    "        \n",
    "    def embed_in_batches(self, chunks, batch_size=16):\n",
    "        num_batches = math.ceil(len(chunks) / batch_size)\n",
    "        embeddings = []\n",
    "        for i in range(num_batches):            \n",
    "            batch = chunks[i*batch_size:i*batch_size+batch_size]\n",
    "            embeddings_batch = self.embedder.embed_documents(batch)\n",
    "            embeddings += embeddings_batch\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_single_document(self, text):\n",
    "        embedding = self.embedder.embed_documents([text])\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "def create_sections(chunks):    \n",
    "    for i, (text, page_num) in enumerate(chunks):\n",
    "        yield {\n",
    "            \"id\":str(i),\n",
    "            \"content\":text,\n",
    "            \"embedding\":embedder.embed_single_document(text)[0],\n",
    "            \"sourcepage\":str(page_num),\n",
    "            \"sourcefile\":filepath.split('/')[-1]\n",
    "        }\n",
    "        \n",
    "def upload_documents(search_client, sections):\n",
    "    \"\"\" \n",
    "    Can refactor to upload in batches\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm \n",
    "    for section in tqdm(sections, total=len(chunks)):\n",
    "        _ = search_client.upload_documents(documents=[section])\n",
    "    print(f\"Documents uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents uploaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # create sections\n",
    "    extractor = PDFExtractor()\n",
    "    chunker = Chunker(chunk_size=500, overlap=100, separator=\" \")\n",
    "    embedder = Embedder(key=openai_key)\n",
    "\n",
    "    text, page_map = extractor.get_document_text(filepath)\n",
    "    chunks = chunker.create_chunks(text, page_map)\n",
    "    sections = create_sections(chunks)        \n",
    "    \n",
    "    # initialize client\n",
    "    search_client = SearchClient(\n",
    "        endpoint = f\"https://{searchservice}.search.windows.net\",\n",
    "        index_name=index, \n",
    "        credential = search_creds\n",
    "    )\n",
    "    \n",
    "    upload_documents(search_client, sections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
